{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116123,"databundleVersionId":13903632,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:44:00.886410Z","iopub.execute_input":"2025-10-20T17:44:00.887009Z","iopub.status.idle":"2025-10-20T17:44:02.888317Z","shell.execute_reply.started":"2025-10-20T17:44:00.886980Z","shell.execute_reply":"2025-10-20T17:44:02.887323Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/Patient-Recovery-Prediction-Challenge/sample_submission.csv\n/kaggle/input/Patient-Recovery-Prediction-Challenge/train.csv\n/kaggle/input/Patient-Recovery-Prediction-Challenge/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Functions to build the tree below","metadata":{}},{"cell_type":"code","source":"import math\nimport random\nimport csv\nimport time\n\n# ====================================================================\n# I. CORE HELPER FUNCTIONS\n# ====================================================================\n\ndef mean(data):\n    \"\"\"Calculates the mean of a list of numerical values.\"\"\"\n    if not data: return 0\n    return sum(data) / len(data)\n\ndef calculate_mse(data):\n    \"\"\"Calculates the Mean Squared Error (MSE) for a regression task.\"\"\"\n    if len(data) == 0: return 0\n    target_values = [row[-1] for row in data]\n    avg = mean(target_values)\n    mse = sum([(val - avg) ** 2 for val in target_values]) / len(target_values)\n    return mse\n\ndef calculate_rmse(predictions, actuals):\n    \"\"\"Calculates the Root Mean Squared Error (RMSE).\"\"\"\n    if len(predictions) != len(actuals) or not predictions: return float('inf')\n    mse = mean([(pred - actual) ** 2 for pred, actual in zip(predictions, actuals)])\n    return math.sqrt(mse)\n\ndef split_data(data, feature_index, threshold):\n    \"\"\"Splits data based on a feature index and a threshold.\"\"\"\n    left = [row for row in data if row[feature_index] <= threshold]\n    right = [row for row in data if row[feature_index] > threshold]\n    return left, right\n\ndef bootstrap_sample(data, subsample_ratio=1.0):\n    \"\"\"Generates a random sample with replacement (bootstrap) using a specified ratio.\"\"\"\n    n_samples_full = len(data)\n    n_samples = int(n_samples_full * subsample_ratio)\n    \n    if n_samples == 0 or n_samples_full == 0: return []\n    \n    sample = []\n    for _ in range(n_samples):\n        sample.append(random.choice(data))\n    return sample\n\n# ====================================================================\n# II. CUSTOM DECISION TREE REGRESSOR\n# ====================================================================\n\nclass Node:\n    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n        self.feature_index = feature_index\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n\nclass RegressionTree:\n    def __init__(self, max_depth=10, min_samples_split=2, min_samples_leaf=1, max_leaf_nodes=None, max_features=None):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.max_leaf_nodes = max_leaf_nodes\n        self.max_features = max_features\n        self.root = None\n        self.leaf_node_count = 0\n\n    def _get_best_split(self, data):\n        best_gain = -1\n        best_split = None\n        if not data: return None\n\n        n_features = len(data[0]) - 1\n        features_to_try = range(n_features)\n        \n        # Feature Selection\n        if self.max_features is not None and self.max_features < n_features:\n            features_to_try = random.sample(range(n_features), self.max_features)\n\n        initial_mse = calculate_mse(data)\n\n        for feature_index in features_to_try:\n            possible_thresholds = sorted(list(set([row[feature_index] for row in data])))\n            \n            for i in range(len(possible_thresholds) - 1):\n                threshold = (possible_thresholds[i] + possible_thresholds[i+1]) / 2 \n\n                left_data, right_data = split_data(data, feature_index, threshold)\n\n                # Check split and leaf constraints\n                if len(left_data) < self.min_samples_split or len(right_data) < self.min_samples_split: continue\n                if len(left_data) < self.min_samples_leaf or len(right_data) < self.min_samples_leaf: continue\n\n                n = len(data)\n                n_left, n_right = len(left_data), len(right_data)\n                weighted_mse = (n_left / n * calculate_mse(left_data)) + (n_right / n * calculate_mse(right_data))\n                gain = initial_mse - weighted_mse\n\n                if gain > best_gain:\n                    best_gain = gain\n                    best_split = {\n                        'feature_index': feature_index,\n                        'threshold': threshold,\n                        'left': left_data,\n                        'right': right_data\n                    }\n        \n        return best_split\n\n    def _build_tree(self, data, current_depth=0):\n        target_values = [row[-1] for row in data]\n        \n        # Check Max Leaf Nodes limit before processing further splits\n        is_max_leaves_reached = self.max_leaf_nodes is not None and self.leaf_node_count >= self.max_leaf_nodes\n\n        # --- BASE CASE: Stop conditions ---\n        if current_depth >= self.max_depth or \\\n           len(data) < self.min_samples_split or \\\n           len(data) < 2 * self.min_samples_leaf or \\\n           len(set(target_values)) <= 1 or \\\n           is_max_leaves_reached: # If global limit reached, force leaf\n            \n            # Create Leaf Node\n            self.leaf_node_count += 1\n            return Node(value=mean(target_values))\n        \n        split = self._get_best_split(data)\n\n        if not split: \n            # If no good split found, create leaf\n            self.leaf_node_count += 1\n            return Node(value=mean(target_values))\n        \n        # --- RECURSIVE STEP ---\n        left_child = self._build_tree(split['left'], current_depth + 1)\n        right_child = self._build_tree(split['right'], current_depth + 1)\n\n        return Node(\n            feature_index=split['feature_index'],\n            threshold=split['threshold'],\n            left=left_child,\n            right=right_child\n        )\n\n    def fit(self, data):\n        self.leaf_node_count = 0 # Reset count for new tree\n        self.root = self._build_tree(data)\n\n    def _predict_one(self, sample, node):\n        if node.value is not None: return node.value \n        \n        feature_value = sample[node.feature_index]\n        \n        if feature_value <= node.threshold:\n            return self._predict_one(sample, node.left)\n        else:\n            return self._predict_one(sample, node.right)\n\n    def predict(self, samples):\n        return [self._predict_one(sample, self.root) for sample in samples]\n\n\n# ====================================================================\n# III. RANDOM FOREST REGRESSOR\n# ====================================================================\n\nclass RandomForestRegressor:\n    def __init__(self, n_trees=100, max_depth=10, min_samples_split=2, min_samples_leaf=1, \n                 max_features='sqrt', max_leaf_nodes=None, subsample_ratio=1.0, \n                 random_data_seed=None, random_feature_seed=None):\n        \n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.max_leaf_nodes = max_leaf_nodes\n        self.subsample_ratio = subsample_ratio\n        self.max_features_setting = max_features\n        self.random_data_seed = random_data_seed\n        self.random_feature_seed = random_feature_seed\n        self.trees = []\n        self.max_features = None\n\n    def fit(self, data):\n        self.trees = []\n        if not data: return\n        \n        if self.random_data_seed is not None:\n            random.seed(self.random_data_seed)\n\n        n_features = len(data[0]) - 1\n        \n        # Calculate max_features integer value\n        if isinstance(self.max_features_setting, str):\n            if self.max_features_setting == 'sqrt':\n                self.max_features = int(math.sqrt(n_features))\n            elif self.max_features_setting == 'log2':\n                self.max_features = int(math.log2(n_features))\n            else:\n                self.max_features = n_features\n        elif isinstance(self.max_features_setting, int):\n            self.max_features = self.max_features_setting\n        else:\n            self.max_features = n_features\n\n        # print(f\"Training {self.n_trees} trees...\")\n        for i in range(self.n_trees):\n            \n            # Set feature seed for reproducible feature selection within this tree\n            if self.random_feature_seed is not None:\n                random.seed(self.random_feature_seed + i) \n            \n            # 1. Bootstrap Sample\n            bootstrapped_data = bootstrap_sample(data, self.subsample_ratio) \n            \n            # 2. Build Tree\n            tree = RegressionTree(\n                max_depth=self.max_depth,\n                min_samples_split=self.min_samples_split,\n                min_samples_leaf=self.min_samples_leaf,\n                max_leaf_nodes=self.max_leaf_nodes,\n                max_features=self.max_features\n            )\n            tree.fit(bootstrapped_data)\n            self.trees.append(tree)\n\n    def predict(self, samples):\n        if not self.trees: raise Exception(\"Model not fitted yet!\")\n            \n        all_predictions = []\n        for tree in self.trees:\n            all_predictions.append(tree.predict(samples))\n        \n        transposed_preds = list(zip(*all_predictions))\n        final_predictions = [mean(sample_preds) for sample_preds in transposed_preds]\n        \n        return final_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:44:12.903803Z","iopub.execute_input":"2025-10-20T17:44:12.904472Z","iopub.status.idle":"2025-10-20T17:44:12.992214Z","shell.execute_reply.started":"2025-10-20T17:44:12.904446Z","shell.execute_reply":"2025-10-20T17:44:12.991279Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Preprocess the data","metadata":{}},{"cell_type":"code","source":"# Function to calculate Root Mean Squared Error (RMSE)\ndef calculate_rmse(predictions, actuals):\n    if len(predictions) != len(actuals) or not predictions:\n        return float('inf')\n    mse = mean([(pred - actual) ** 2 for pred, actual in zip(predictions, actuals)])\n    return math.sqrt(mse)\n\ndef load_and_preprocess(file_path, is_train=True):\n    \"\"\"\n    Loads data, extracts core features, adds 7 new engineered features, \n    and returns features (+ target if is_train=True).\n    Assumes CSV structure: ID, F1, F2, F3, F4, F5, [TARGET]\n    \"\"\"\n    data = []\n    \n    # Epsilon to prevent division by zero\n    EPSILON = 1e-6 \n    \n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        header = next(reader) # Skip header row\n        \n        for row in reader:\n            try:\n                # 1. Extract Core Features (Indices 1 to 5)\n                therapy_hours = float(row[1])      # F1\n                health_score = float(row[2])       # F2\n                lifestyle = 1.0 if row[3] == 'Yes' else 0.0 # F3\n                sleep_hours = float(row[4])        # F4\n                follow_up_sessions = float(row[5]) # F5\n                \n                # Start feature list with core numerical features (5 features)\n                features = [therapy_hours, health_score, lifestyle, sleep_hours, follow_up_sessions]\n                \n                # 2. FEATURE ENGINEERING (Adding 7 New Features)\n                \n                # E1: Therapy-Session Load (Interaction)\n                features.append(therapy_hours * follow_up_sessions) \n                \n                # E2: Therapy-to-Health Ratio (Ratio)\n                features.append(therapy_hours / (health_score + EPSILON))\n\n                # E3: Therapy-to-Sleep Ratio (Ratio)\n                features.append(therapy_hours / (sleep_hours + EPSILON))\n                \n                # E4: High Sessions Flag (Threshold) - Using 15 as a high threshold\n                features.append(1.0 if follow_up_sessions > 15 else 0.0) \n                \n                # E5: Sleep-Lifestyle Harmony (Interaction)\n                features.append(sleep_hours * lifestyle)\n                \n                # E6: Health-to-Sleep Ratio (Ratio)\n                features.append(health_score / (sleep_hours + EPSILON))\n\n                # E7: Sleep Squared (Transformation for non-linearity)\n                features.append(sleep_hours ** 2)\n                \n                # 3. Add Target for Training\n                if is_train:\n                    target = float(row[6])\n                    data.append(features + [target])\n                else:\n                    # For test data, just append features\n                    data.append(features)\n            \n            except (ValueError, IndexError) as e:\n                print(f\"Error processing row: {row}. Skipping. Error: {e}\")\n                \n    return data\n\ndef grid_search_tuning(train_set, val_features, val_targets, param_grid):\n    best_rmse = float('inf')\n    best_params = {}\n    \n    print(\"\\n--- Starting Grid Search Tuning ---\")\n    print(f\"Total combinations to test: {len(param_grid)}\")\n\n    start_time = time.time()\n    \n    for i, params in enumerate(param_grid):\n        # Time each run to track efficiency\n        run_start = time.time()\n        \n        # Ensure random seed is set for data sampling and feature selection\n        if 'random_data_seed' not in params: params['random_data_seed'] = 42\n        if 'random_feature_seed' not in params: params['random_feature_seed'] = 100\n        \n        print(f\"\\n[{i+1}/{len(param_grid)}] Testing: {params}\")\n        \n        try:\n            rf = RandomForestRegressor(**params)\n            rf.fit(train_set)\n            \n            val_predictions = rf.predict(val_features)\n            rmse = calculate_rmse(val_predictions, val_targets)\n            \n            run_time = time.time() - run_start\n            print(f\"  -> Validation RMSE: {rmse:.4f} (Time: {run_time:.2f}s)\")\n\n            if rmse < best_rmse:\n                best_rmse = rmse\n                best_params = params\n                print(\"  (NEW BEST MODEL FOUND!)\")\n                \n        except Exception as e:\n            print(f\"  ERROR during training/prediction: {e}\")\n            \n    total_time = time.time() - start_time\n    \n    print(\"\\n--- Grid Search Complete ---\")\n    print(f\"Total search time: {total_time:.2f} seconds\")\n    return best_params, best_rmse\n\n# --- EXECUTION ---\n\n# !!! CRITICAL: UPDATE THIS PATH !!!\nTRAIN_FILE = '/kaggle/input/Patient-Recovery-Prediction-Challenge/train.csv'\n\ntry:\n    full_train_data = load_and_preprocess(TRAIN_FILE, is_train=True)\nexcept FileNotFoundError:\n    print(f\"ERROR: Train file not found at {TRAIN_FILE}. Please correct the path.\")\n    raise\n\n# Set seed for reproducible data split\nrandom.seed(42) \nrandom.shuffle(full_train_data)\nsplit_ratio = 0.8\nsplit_point = int(len(full_train_data) * split_ratio)\n\ntrain_data = full_train_data[:split_point]\nvalidation_data = full_train_data[split_point:]\n\nval_features = [row[:-1] for row in validation_data]\nval_targets = [row[-1] for row in validation_data]\n\nprint(f\"Total records: {len(full_train_data)} | Train: {len(train_data)} | Validation: {len(validation_data)}\")\nprint(f\"Number of features (Core + Engineered): {len(train_data[0]) - 1}\") # Should be 12 features\n\n\n# Assuming 'fixed_params', 'grid_search_tuning', and the data are loaded/defined above.\n\n# --- AGGRESSIVE HIGH-RESOLUTION PARAMETER DEFINITIONS ---\nfine_n_trees = [100]\nfine_max_depth = [14, 17] \nfine_max_features = [4, 6, 8, 10] \nfine_min_leaf = [3, 5]\nfine_subsample_ratio = [0.85] # Introducing subsample ratio into the loop\nfine_max_leaf_nodes = [None] \n\nparam_grid_aggressive = []\nrandom_seed = 42\n\n# Re-establish a sensible 'fixed_params':\nfixed_params = {\n    'min_samples_split': 6,\n    'random_data_seed': 42,\n    'random_feature_seed': 100\n}\n\n# --- GENERATE AGGRESSIVE GRID (2 * 3 * 4 * 2 * 2 * 1 = 96 Combinations) ---\nfor n_t in fine_n_trees:\n    for m_d in fine_max_depth:\n        for m_f in fine_max_features:\n            for m_l in fine_min_leaf:\n                for s_r in fine_subsample_ratio:\n                    for M_L_N in fine_max_leaf_nodes:\n                        params = fixed_params.copy()\n                        params.update({\n                            'n_trees': n_t,\n                            'max_depth': m_d,\n                            'max_features': m_f,\n                            'min_samples_leaf': m_l,\n                            'subsample_ratio': s_r,\n                            'max_leaf_nodes': M_L_N\n                        })\n                        param_grid_aggressive.append(params)\n\nprint(f\"Total combinations for Aggressive Retuning Grid: {len(param_grid_aggressive)}\")\n\n# --- EXECUTE THE GRID SEARCH ---\n# NOTE: This assumes 'train_data', 'val_features', and 'val_targets' \n# have been correctly loaded with the new 12 features.\nbest_params, best_rmse = grid_search_tuning(train_data, val_features, val_targets, param_grid_aggressive)\n\nprint(\"\\n=========================================================\")\nprint(\"  FINAL BEST MODEL RESULTS (Aggressive Retuning with 12 Features)\")\nprint(f\"  Best Parameters: {best_params}\")\nprint(f\"  Best Validation RMSE: {best_rmse:.4f}\")\nprint(\"=========================================================\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:44:25.336098Z","iopub.execute_input":"2025-10-20T17:44:25.336426Z"}},"outputs":[{"name":"stdout","text":"Total records: 8000 | Train: 6400 | Validation: 1600\nNumber of features (Core + Engineered): 12\nTotal combinations for Aggressive Retuning Grid: 16\n\n--- Starting Grid Search Tuning ---\nTotal combinations to test: 16\n\n[1/16] Testing: {'min_samples_split': 6, 'random_data_seed': 42, 'random_feature_seed': 100, 'n_trees': 100, 'max_depth': 14, 'max_features': 4, 'min_samples_leaf': 3, 'subsample_ratio': 0.85, 'max_leaf_nodes': None}\n  -> Validation RMSE: 2.1511 (Time: 171.99s)\n  (NEW BEST MODEL FOUND!)\n\n[2/16] Testing: {'min_samples_split': 6, 'random_data_seed': 42, 'random_feature_seed': 100, 'n_trees': 100, 'max_depth': 14, 'max_features': 4, 'min_samples_leaf': 5, 'subsample_ratio': 0.85, 'max_leaf_nodes': None}\n  -> Validation RMSE: 2.1535 (Time: 169.26s)\n\n[3/16] Testing: {'min_samples_split': 6, 'random_data_seed': 42, 'random_feature_seed': 100, 'n_trees': 100, 'max_depth': 14, 'max_features': 6, 'min_samples_leaf': 3, 'subsample_ratio': 0.85, 'max_leaf_nodes': None}\n  -> Validation RMSE: 2.1401 (Time: 242.25s)\n  (NEW BEST MODEL FOUND!)\n\n[4/16] Testing: {'min_samples_split': 6, 'random_data_seed': 42, 'random_feature_seed': 100, 'n_trees': 100, 'max_depth': 14, 'max_features': 6, 'min_samples_leaf': 5, 'subsample_ratio': 0.85, 'max_leaf_nodes': None}\n  -> Validation RMSE: 2.1473 (Time: 242.22s)\n\n[5/16] Testing: {'min_samples_split': 6, 'random_data_seed': 42, 'random_feature_seed': 100, 'n_trees': 100, 'max_depth': 14, 'max_features': 8, 'min_samples_leaf': 3, 'subsample_ratio': 0.85, 'max_leaf_nodes': None}\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import csv\n# (Ensure RandomForestRegressor and load_and_preprocess are defined in your notebook)\n\n# --- 1. SETUP: FINAL TRAINED MODEL AND PATHS ---\n\n# You must redefine your final parameters here based on your BEST TUNING RESULT (RMSE: 2.1275)\nfinal_params = {\n    'max_features': 3, 'subsample_ratio': 0.85, 'min_samples_split': 6, \n    'min_samples_leaf': 5, 'random_data_seed': 42, 'random_feature_seed': 100, \n    'n_trees': 250, 'max_depth': 13, 'max_leaf_nodes': None\n}\n\n# Fix 1: Correct TRAIN_FILE path (assuming this is the training data path)\nTRAIN_FILE = '/kaggle/input/Patient-Recovery-Prediction-Challenge/train.csv'\n\n# --- CORRECTED TRAINING DATA LOAD ---\ntry:\n    # FIX: is_train must be True to load the target column required for training\n    full_train_data = load_and_preprocess(TRAIN_FILE, is_train=True) \n    print(f\"Loaded {len(full_train_data)} records for final training.\")\nexcept FileNotFoundError:\n    print(f\"ERROR: Train file not found at {TRAIN_FILE}. Please correct the path.\")\n    raise\n\n# 1. Instantiate the final model\nfinal_rf = RandomForestRegressor(**final_params)\n\n# 2. Train on ALL available training data\nprint(\"Starting FINAL training...\")\nfinal_rf.fit(full_train_data) # This will now work as full_train_data has the target\nprint(\"Final model successfully trained.\")\n\n# !!! CRITICAL: UPDATE THIS PATH TO YOUR KAGGLE TEST FILE LOCATION !!!\nTEST_FILE = '/kaggle/input/Patient-Recovery-Prediction-Challenge/test.csv' \n\n\n# --- 2. FUNCTION TO LOAD TEST DATA (Includes ID Handling) ---\n\ndef load_test_data(file_path):\n    \"\"\"Loads test data, extracting IDs and features using indices consistent with training.\"\"\"\n    test_features = []\n    test_ids = []\n    \n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        header = next(reader)  # Skip header\n        \n        # Assumption based on error fixing: Test file has an ID column at index 0.\n        \n        for row in reader:\n            test_ids.append(row[0])  # ID is the first column (index 0)\n            \n            try:\n                # Features start at index 1 (F1=Therapy)\n                \n                # Therapy Hours (1), Initial Health Score (2)\n                features = [float(row[1]), float(row[2])] \n                \n                # Lifestyle Activities (3): 'Yes' -> 1.0, 'No' -> 0.0\n                features.append(1.0 if row[3] == 'Yes' else 0.0)\n                \n                # Average Sleep Hours (4), Follow-Up Sessions (5)\n                features.extend([float(row[4]), float(row[5])])\n                \n                test_features.append(features)\n                \n            except (ValueError, IndexError) as e:\n                print(f\"Error processing row: {row}. Check test file column indices. Error: {e}\")\n                \n    return test_ids, test_features\n\n\n# --- 3. EXECUTION: LOAD, PREDICT, AND SUBMIT ---\n\nprint(\"Starting prediction process...\")\n\n# Load Test Data\ntry:\n    test_ids, test_features = load_test_data(TEST_FILE)\n    print(f\"Successfully loaded {len(test_features)} test samples.\")\nexcept FileNotFoundError:\n    print(f\"FATAL ERROR: Test file not found at {TEST_FILE}. FIX THE PATH.\")\n    raise\n\n# Generate Predictions\ntest_predictions = final_rf.predict(test_features)\n\n# Round the predictions to the nearest integer (standard for many Kaggle regression targets)\nfinal_predictions_rounded = [round(p) for p in test_predictions]\n\n# Create Submission File\nSUBMISSION_FILE = 'submission.csv'\nsubmission_rows = []\nfor patient_id, prediction in zip(test_ids, final_predictions_rounded):\n    submission_rows.append([patient_id, prediction])\n\nwith open(SUBMISSION_FILE, 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow(['Id', 'Recovery Index']) # Standard Kaggle submission header\n    writer.writerows(submission_rows)\n\nprint(\"\\n------------------------------------------------------\")\nprint(f\"âœ… Submission file '{SUBMISSION_FILE}' created successfully.\")\nprint(f\"Predictions generated for {len(submission_rows)} patients.\")\nprint(\"Your custom Random Forest project is complete! You can now submit this file.\")\nprint(\"------------------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:12:46.010709Z","iopub.execute_input":"2025-10-20T17:12:46.011251Z","iopub.status.idle":"2025-10-20T17:14:41.722716Z","shell.execute_reply.started":"2025-10-20T17:12:46.011220Z","shell.execute_reply":"2025-10-20T17:14:41.721766Z"}},"outputs":[],"execution_count":null}]}